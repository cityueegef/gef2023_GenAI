{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raVCSGpCD7Pi"
   },
   "source": [
    "# Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "iDpNpym_D7Pk",
    "outputId": "a0a9059b-3a40-478d-b798-f6c551601045"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "    \n",
    "from lib.envs.cliff_walking import CliffWalkingEnv\n",
    "from lib.simulation import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zx9J54BZD7P0"
   },
   "outputs": [],
   "source": [
    "class Agent(object):  \n",
    "        \n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.num_actions = len(actions)\n",
    "\n",
    "    def act(self, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeXHUlt8D7QB"
   },
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    \n",
    "    def __init__(self, actions, epsilon=0.01, alpha=0.5, gamma=1):\n",
    "        super(QLearningAgent, self).__init__(actions)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = {}\n",
    "        self.sigma = 10\n",
    "       \n",
    "    def stateToString(self, state):\n",
    "        mystring = \"\"\n",
    "        if np.isscalar(state):\n",
    "            mystring = str(state)\n",
    "        else:\n",
    "            for digit in state:\n",
    "                mystring += str(digit)\n",
    "        return mystring    \n",
    "    \n",
    "    def act(self, state):\n",
    "        \n",
    "        stateStr = self.stateToString(state)    \n",
    "        random_action = np.random.randint(0, self.num_actions) \n",
    "        \n",
    "        if stateStr not in self.Q:\n",
    "            best_action = random_action\n",
    "            init_Q = np.random.normal(0,self.sigma)\n",
    "            self.Q.update({ stateStr: np.zeros(self.num_actions) })\n",
    "        else:\n",
    "            qs = self.Q[stateStr]\n",
    "            choices = np.where(qs==np.max(qs))[0]\n",
    "            if len(choices)>1:\n",
    "                best_action = np.random.choice(choices)\n",
    "            else:\n",
    "                best_action = choices[0]      \n",
    "        \n",
    "\n",
    "        ## Epsilon greedy policy        \n",
    "        action = np.random.choice([random_action, best_action], p=np.array([ self.epsilon, 1-self.epsilon]))\n",
    "\n",
    "            \n",
    "        assert action == 0 or action == 1 or action == 2 or action == 3\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state1, action1, reward, state2, done):\n",
    "       \n",
    "        state1Str = self.stateToString(state1)\n",
    "        state2Str = self.stateToString(state2)\n",
    "        \n",
    "        \n",
    "        if state1Str not in self.Q:\n",
    "            init_Q = np.random.normal(0,self.sigma)\n",
    "            self.Q.update({ state1Str: np.zeros(self.num_actions) })\n",
    "            Q_current_s = self.Q[state1Str]\n",
    "        else:\n",
    "            Q_current_s = self.Q[state1Str][int(action1)]\n",
    "             \n",
    "                \n",
    "        if state2Str not in self.Q:\n",
    "\n",
    "            init_Q = np.random.normal(0,self.sigma)\n",
    "            self.Q.update({ state2Str: np.zeros(self.num_actions) })\n",
    "            Q_new_s = self.Q[state2Str]\n",
    "        else:\n",
    "            Q_new_s = self.Q[state2Str]\n",
    "\n",
    "\n",
    "        ## Q-learning update\n",
    "        qmax = max([i - Q_current_s  for i in Q_new_s.tolist()])\n",
    "        final_Q = Q_current_s + self.alpha * (reward + self.gamma * qmax)\n",
    "        self.Q[state1Str][int(action1)] = final_Q\n",
    "        \n",
    "        \"\"\"\n",
    "        Q-learning Update:\n",
    "        Q(s,a) <- Q(s,a) + alpha * (reward + gamma * max(Q(s') - Q(s,a))\n",
    "        or\n",
    "        Q(s,a) <- Q(s,a) + alpha * (td_target - Q(s,a))\n",
    "        or\n",
    "        Q(s,a) <- Q(s,a) + alpha * td_delta\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "aEEFjnAED7QT",
    "outputId": "c2bd895c-58ef-4ab3-c7c2-17ce9dc6e979",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interactive = True\n",
    "%matplotlib nbagg\n",
    "env = CliffWalkingEnv()\n",
    "agent = QLearningAgent(range(env.action_space.n), epsilon=0.01)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_qlearning(100, interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgCWAZp5D7Qb",
    "outputId": "c452ba1f-2b86-448c-869e-85d96e5f5c9d"
   },
   "outputs": [],
   "source": [
    "interactive = False\n",
    "%matplotlib inline\n",
    "env = CliffWalkingEnv()\n",
    "agent = QLearningAgent(range(env.action_space.n), epsilon=0.01)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_qlearning(2000, interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iD1VIRrxD7Qg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Q-Learning Agent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
